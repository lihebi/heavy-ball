#+TITLE: Heavy Ball implementation in Emane

* Introduction
** cite:2016-INFOCOM-Liu-Heavy Heavy-Ball: A New Approach for Taming Delay and Convergence in Wireless Network Optimization

* TODO-list
** DONE run Zehua's VM
   CLOSED: [2019-10-23 Wed 20:28]
1. start emane with schedule-1hop.xml, by

demo-start
emaneevent-tdmaschedule xxx.xml -i emanenode0

** DONE test privileged docker
   CLOSED: [2019-10-24 Thu 14:14]
Whehter LXC works inside such docker
** CANCELED Test LXD distrobuilder
   CLOSED: [2019-10-24 Thu 14:15]
** Fix the PYTHONPATH problem
ubuntu/debian uses dist-packages for apt-installed python modules,
however, when using setup.py (as of emane), it seems to install in
site-packages, which is not in sys.path. Further, it seems that
.local/.../site-packages/ is added to sys.path somehow during pip
install, so is the sys.path updatable?

** deploy a local version
The project depends on python2, both interpreter and header
file. Thus, I'd better have a legacy environment to run it?

** understand the workflow
** refactor out into shim layer
*** what is shim layer exactly?
** multi-hop network

* Other References
- a emane docker setup: https://github.com/savagesmc/emane_docker
- another docker setup (with CORE): https://github.com/devdkerr/core
  - also on dockerhub: https://hub.docker.com/r/devdkerr/core/
  - has a paper: Comparison of CORE Network Emulation Platforms,
    Proceedings of IEEE MILCOM Conference, 2010, pp.864-869.

- Make Operating System Image: https://github.com/systemd/mkosi

* Code reference
** EMANE stack
 - EMANE: https://github.com/adjacentlink/emane
 - emane-tutorial: https://github.com/adjacentlink/emane-tutorial
 - MGEN: https://github.com/USNavalResearchLaboratory/mgen
 - CORE: https://github.com/coreemu/core
 - iperf: https://github.com/esnet/iperf
 - olsr: https://github.com/USNavalResearchLaboratory/nrlolsr


** Project code
From Zehua:
- https://github.com/zehuali/emane/commits/tdma-develop
  - https://github.com/zehuali/emane/tdma-develop
- https://git.ece.iastate.edu/zehuali/emane-tutorial
- https://git.ece.iastate.edu/jlwahlig/CentralizedScheduler

- SNR?
- Pathloss?
- CSI?

** My fork
- emane: https://github.com/lihebi/emane/

use tdma-develop branch

- tutorial: I'm going to just use this
  https://git.ece.iastate.edu/zehuali/emane-tutorial, as I'm probably
  moving out. But to avoid accidental push:
  - https://git.ece.iastate.edu/hebi/emane-tutorial

- multi-hop https://git.ece.iastate.edu/jlwahlig/CentralizedScheduler


* VMs

** LXC

Creating the container. I'm using --no-validate because I cannot
connect to the default gpg server successfully.

#+begin_example
sudo lxc-create -t download -n playtime -- --dist ubuntu --release xenial --arch amd64 --no-validate
sudo lxc-create -t download -n playtime -- --dist archlinux --release current --arch amd64 --no-validate
#+end_example

Start the container:
#+begin_example
sudo lxc-start -n playtime
#+end_example

*** default configurations
=/etc/lxc/default.conf=, as a reference.

The default ubuntu configuration

#+begin_example
lxc.net.0.type = veth
lxc.net.0.link = lxcbr0
lxc.net.0.flags = up
lxc.net.0.hwaddr = 00:16:3e:xx:xx:xx
#+end_example

The default debian configuration

#+begin_example
lxc.net.0.type = empty
lxc.apparmor.profile = generated
lxc.apparmor.allow_nesting = 1
#+end_example

To maintain a good default conf:

#+begin_example
lxc.net.0.type = empty
lxc.net.0.link = lxcbr0
lxc.net.0.flags = up
lxc.net.0.hwaddr = 00:16:3e:xx:xx:xx
lxc.cgroup.devices.allow =
lxc.cgroup.devices.deny =
#+end_example

*** Failed to unshare CLONE_NEWNS

#+begin_example
lxc-create: test1: lxccontainer.c: create_run_template: 1349 Failed to unshare CLONE_NEWNS
lxc-create: test1: lxccontainer.c: create_run_template: 1617 Failed to create container from template
lxc-create: test1: tools/lxc_create.c: main: 327 Failed to create container test1
#+end_example

https://github.com/lxc/lxc/issues/3003 run docker in privileged mode

#+begin_example
docker run -it --privileged ubuntu
#+end_example

*** Failed to setup limits for the "devices" controller
#+begin_example
lxc-start playtime3 20191025152203.516 ERROR    cgfsng - cgroups/cgfsng.c:cg_legacy_set_data:2191 - Failed to setup limits for the "devices" controller. The controller seems to be unused by "cgfsng" cgroup driver or not enabled on the cgroup hierarchy
lxc-start playtime3 20191025152203.516 ERROR    start - start.c:lxc_spawn:1802 - Failed to setup legacy device cgroup controller limits
lxc-start playtime3 20191025152203.516 ERROR    lxccontainer - lxccontainer.c:wait_on_daemonized_start:842 - Received container state "ABORTING" instead of "RUNNING"
lxc-start playtime3 20191025152203.516 ERROR    lxc_start - tools/lxc_start.c:main:330 - The container failed to start
lxc-start playtime3 20191025152203.516 ERROR    lxc_start - tools/lxc_start.c:main:333 - To get more details, run the container in foreground mode
lxc-start playtime3 20191025152203.516 ERROR    lxc_start - tools/lxc_start.c:main:336 - Additional information can be obtained by setting the --logfile and --logpriority options
lxc-start playtime3 20191025152203.516 ERROR    start - start.c:__lxc_start:1939 - Failed to spawn container "playtime3"
#+end_example


According to https://github.com/lxc/lxc/issues/2268, I need to add to
/etc/lxc/default.conf the following:

#+begin_example
lxc.cgroup.devices.allow =
lxc.cgroup.devices.deny =
#+end_example

Then recreate the VM and start it:

*** Failed to attach "lxcbr0" to openvswitch bridge "vethC01WGR"

#+begin_quote
lxc-start playtime 20191025151905.114 ERROR    utils - utils.c:run_command:1615 - Failed to exec command
lxc-start playtime 20191025151905.114 ERROR    network - network.c:lxc_ovs_attach_bridge:1887 - Failed to attach "lxcbr0" to openvswitch bridge "vethC01WGR": lxc-start: playtime: utils.c: run_c
ommand: 1615 Failed to exec command
lxc-start playtime 20191025151905.114 ERROR    network - network.c:instantiate_veth:172 - Operation not permitted - Failed to attach "vethC01WGR" to bridge "lxcbr0"
lxc-start playtime 20191025151905.134 ERROR    network - network.c:lxc_create_network_priv:2457 - Failed to create network device
lxc-start playtime 20191025151905.134 ERROR    start - start.c:lxc_spawn:1626 - Failed to create the network
lxc-start playtime 20191025151905.134 ERROR    start - start.c:__lxc_start:1939 - Failed to spawn container "playtime"
lxc-start playtime 20191025151905.134 ERROR    lxccontainer - lxccontainer.c:wait_on_daemonized_start:842 - Received container state "STOPPING" instead of "RUNNING"
lxc-start playtime 20191025151905.134 ERROR    lxc_start - tools/lxc_start.c:main:330 - The container failed to start
lxc-start playtime 20191025151905.134 ERROR    lxc_start - tools/lxc_start.c:main:333 - To get more details, run the container in foreground mode
lxc-start playtime 20191025151905.134 ERROR    lxc_start - tools/lxc_start.c:main:336 - Additional information can be obtained by setting the --logfile and --logpriority options
#+end_quote

This error is now shown on Debian, so compare the configurations, the
default ubuntu configuration has:

#+begin_example
lxc.net.0.type = veth
#+end_example

change it to

#+begin_example
lxc.net.0.type = empty
#+end_example

If I need some networks, this might not work. A side note, ubuntu does
not have lxc and lxc-net daemon, while debian has.


** LXD

It actually support a declarative approach to build VM, using
https://github.com/lxc/distrobuilder. But this seems to be very new,
the only release (1.0) out 3 days ago (10/21/2019). It uses a YAML as
input. See some examples:
- doc/examples in lxc/distrobuilder repo
- https://github.com/lxc/lxc-ci, the images/ folder

#+begin_quote
It's the replacement of the LXC template scripts and has slowly been
taking over the generation of the many pre-built images that LXC and
LXD consume.
#+end_quote

The official list of images:
- https://us.images.linuxcontainers.org
- build farm CI: https://jenkins.linuxcontainers.org/view/Images/

Many of the LXD files use debootstrap as a base. As a side note, to
install a OS into a partition, from a host OS, debian has
[[https://wiki.debian.org/Debootstrap][debootstrap]], arch has
=pacstrap= (which seems to be
[[https://git.archlinux.org/arch-install-scripts.git/][arch-install-scripts]])
and [[https://github.com/tokland/arch-bootstrap][arch-bootstrap]].

One potential problem is that, the examples are only for building
different distros, thus it is not clear if it supports FROM xxx to
reuse an existing image declaration.

* Building Emane

Dependencies:
#+begin_example
libxml2
libpcap
pcre
libuuid
protobuf
python-protobuf
python-lxml
#+end_example


In ubuntu:
#+begin_example
libxml2-dev libpcap-dev libpcre3 uuid-dev protobuf-compiler libprotobuf-dev python-protobuf python-lxml
#+end_example

NOT:
#+begin_example
libuuid1
#+end_example

Additional dependencies:

#+begin_example
libtool
#+end_example

** configure prefix

It seems that during configure, I have to set prefix to =/usr=,
otherwise during installation, because the tutorials have
/usr/share/emane/xxx.dtd fixed in all the xml files. However, it is
weird that when setting prefix /usr, the python packages will be
installed in /usr/lib/python2.7/site-packages, which is not in
sys.path. When using default /usr/local prefix however, it is
installed in /usr/local/lib/python2.7/dist-packages, which is in
sys.path.

So currently I just decide to install emane python module manually. It
is also possible to use both prefix to install two copies, but this is
not clean.

* Emane tutorial Dependencies

The =ip= command is in

#+begin_example
iproute2
#+end_example

To start the GUI, looks like I need:

#+begin_example
pip3 install pyqt5
#+end_example

This is giving me errors. I probably need to install from apt

#+begin_example
apt install python3-pyqt5
#+end_example

Also, I need to modify =8/gui/main.py= for the fixed
=/home/emane/Development/tutorial= path.

** Other applications
#+begin_example
apt install gpsd gpsd-clients olsrd iperf iperf3
#+end_example

https://github.com/adjacentlink/pynodestatviz, probably make from source.

** TODO mgen
I probably also need to install mgen. There is a ubuntu package for it

#+begin_example
apt install mgen
#+end_example

I probably need to use a custom built mgen? But Zehua does not seem to
modify mgen.


** opentestpoint
These tutorials also need to the command =otestpoint-broker=, which is
https://github.com/adjacentlink/opentestpoint

To build opentestpoint, I need additional dependencies:

#+begin_example
sqlite
zeromq
python-devel
#+end_example

which in Ubuntu is

#+begin_example
python-dev libsqlite3-dev libczmq-dev
#+end_example

It also depends on

#+begin_example
python-setuptools
#+end_example

https://github.com/adjacentlink/opentestpoint-probe-emane

* Solved Problems
** DONE LXC
   CLOSED: [2019-10-24 Thu 14:15]
It needs lxc:

#+begin_example
apt install lxc
#+end_example

There might be problems running lxc inside docker.

The problem

#+begin_example
brctl addbr mybr0
#+end_example

is not working, with following errors:

#+begin_example
add bridge failed: Operation not permitted
#+end_example

This is due to permission problem, as docker is not running
full-privileged. I can verify on the host, without sudo, it is giving
the same error, but it works with sudo. So create docker with
privileged:

#+begin_example
sudo docker run --privileged --rm -it hebivm
#+end_example

And inside docker, if running as root, it works. However, if running
as user via sudo, it seems to work because the bridge is
created. However, the following error messages:

#+begin_example
[docker] ~ >>> $ sudo brctl addbr mybr0
PAM-CGFS[513]: Failed to get list of controllers

sudo[513]: pam_unix(sudo:session): session closed for user root
PAM-CGFS[513]: Failed to get list of controllers
#+end_example

I have no idea why, and I have no idea whether I can assume this
problem is solved on docker side. If not, I might consider run LXC as
VM.

Fortunately, the lxc bridge inside docker seems to be containized as
well, i.e. the bridges are not conflicting from the host and different
container instances.

* DONE-List
** DONE move all documents here
   CLOSED: [2019-10-24 Thu 12:12]

** DONE switch to VM
   CLOSED: [2019-10-23 Wed 20:28]
